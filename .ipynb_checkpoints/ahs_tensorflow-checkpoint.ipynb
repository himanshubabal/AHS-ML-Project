{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "\n",
    "# from keras.models import Sequential\n",
    "# from keras.layers import Dense, Dropout, Activation\n",
    "# from keras.optimizers import SGD\n",
    "# from keras.utils.np_utils import to_categorical\n",
    "\n",
    "import numpy as np\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.mlab as mlab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "data_path = 'data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.0.0'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   \n",
      "Splitting train and test data in ratio 85:15\n"
     ]
    }
   ],
   "source": [
    "diagnosed_data = pd.read_csv(data_path + '22_COMB_diag_hotData.csv', low_memory=False)\n",
    "diagnosed_col = pd.read_csv(data_path + '22_COMB_diag_col.csv', low_memory=False)\n",
    "\n",
    "if 'Unnamed: 0' in list(diagnosed_col):\n",
    "\tdiagnosed_col = diagnosed_col.drop('Unnamed: 0',axis=1,errors='ignore')\n",
    "\n",
    "assert (diagnosed_data.shape[0] == diagnosed_col.shape[0])\n",
    "split_index = int(diagnosed_data.shape[0] * 0.85)\n",
    "\n",
    "print('   ')\n",
    "print('Splitting train and test data in ratio 85:15')\n",
    "train_data = np.array(diagnosed_data.astype(float))[:split_index]\n",
    "train_label = np.array(diagnosed_col.astype(float))[:split_index][:,0]\n",
    "\n",
    "test_data = np.array(diagnosed_data.astype(float))[split_index:]\n",
    "test_label = np.array(diagnosed_col.astype(float))[split_index:][:,0]"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 8,
=======
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((158036, 303), (158036, 1))\n"
     ]
    }
   ],
   "source": [
    "print(diagnosed_data.shape, diagnosed_col.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
>>>>>>> refs/remotes/origin/himanshu
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((158036, 303), (158036, 1))\n"
     ]
    }
   ],
   "source": [
    "print(diagnosed_data.shape, diagnosed_col.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "pred_list = [1.0,2.0,3.0,7.0,9.0,19.0,21.0,99.0]\n",
    "new_list = [1,2,3,4,5,6,7,8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Replace Label No 99 by 32\n",
    "# Label No 99 causes 'to_categorical' to make 100 one-hot values\n",
    "# Replacing it by 33 leads to only 33 values\n",
    "def replace_99_labes(label_data):\n",
    "    for i in range(len(label_data)):\n",
    "        e = label_data[i]\n",
    "        if e in pred_list:\n",
    "            ind = pred_list.index(label_data[i])\n",
    "            new_element = new_list[ind]\n",
    "            label_data[i] = new_element\n",
    "        else:\n",
    "            label_data[i] = 0\n",
    "# \t\tif label_data[i] == 99.0 :\n",
    "# \t\t\tlabel_data[i] = 32.0\n",
    "    return label_data"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
=======
   "execution_count": 132,
   "metadata": {
    "collapsed": false
>>>>>>> refs/remotes/origin/himanshu
   },
   "outputs": [],
   "source": [
    "to_keep_value = 1.0\n",
    "\n",
    "def binary_labels(label_data):\n",
    "    for i in range(len(label_data)):\n",
    "        if label_data[i] != 1.0 :\n",
    "            label_data[i] = 0.0\n",
    "# \t\tif label_data[i] == 99.0 :\n",
    "# \t\t\tlabel_data[i] = 32.0\n",
    "    return label_data"
<<<<<<< HEAD
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "train_rep = replace_99_labes(train_label)\n",
    "test_rep = replace_99_labes(test_label)\n",
    "\n",
    "# train_rep = binary_labels(train_label)\n",
    "# test_rep = binary_labels(test_label)"
=======
>>>>>>> refs/remotes/origin/himanshu
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 13,
=======
   "execution_count": 133,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# train_rep = replace_99_labes(train_label)\n",
    "# test_rep = replace_99_labes(test_label)\n",
    "\n",
    "train_rep = binary_labels(train_label)\n",
    "test_rep = binary_labels(test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
>>>>>>> refs/remotes/origin/himanshu
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((134330, 303), (134330,))\n",
      "((23706, 303), (23706,))\n"
     ]
    }
   ],
   "source": [
    "print(train_data.shape, train_rep.shape)\n",
    "print(test_data.shape, test_rep.shape)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 14,
=======
   "execution_count": 135,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.])"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_rep[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def acc(predictions, labels):\n",
    "    return (100.0 * np.sum(np.argmax(predictions, 2).T == labels) / predictions.shape[1] / predictions.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "def onehot_labels(labels):\n",
    "    print(labels.shape)\n",
    "    if len(labels.shape) < 2:\n",
    "        labels = labels.reshape(-1,1)\n",
    "    enc = OneHotEncoder()\n",
    "    return (enc.fit_transform(labels).toarray())\n",
    "\n",
    "# (134330,1)  -->  (134330, 9)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(23706,)\n",
      "(134330,)\n",
      "((134330, 2), (23706, 2))\n"
     ]
    }
   ],
   "source": [
    "test_label_hot = onehot_labels(test_rep)\n",
    "train_label_hot = onehot_labels(train_rep)\n",
    "\n",
    "print(train_label_hot.shape, test_label_hot.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
>>>>>>> refs/remotes/origin/himanshu
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 6.,  0.,  8.,  4.,  7.,  0.,  0.,  4.,  1.,  7.])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_rep[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def acc(predictions, labels):\n",
    "    return (100.0 * np.sum(np.argmax(predictions, 2).T == labels) / predictions.shape[1] / predictions.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "def onehot_labels(labels):\n",
    "    print(labels.shape)\n",
    "    if len(labels.shape) < 2:\n",
    "        labels = labels.reshape(-1,1)\n",
    "    enc = OneHotEncoder()\n",
    "    return (enc.fit_transform(labels).toarray())\n",
    "\n",
    "# (134330,1)  -->  (134330, 9)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "(23706,)\n",
      "(134330,)\n",
      "((134330, 9), (23706, 9))\n"
=======
      "('Y1 : ', TensorShape([Dimension(None), Dimension(100)]))\n",
      "('Y2 : ', TensorShape([Dimension(None), Dimension(75)]))\n",
      "('Y3 : ', TensorShape([Dimension(None), Dimension(50)]))\n",
      "('YF : ', TensorShape([Dimension(None), Dimension(2)]))\n",
      "('Yf : ', TensorShape([Dimension(None), Dimension(2)]))\n",
      "('YY : ', TensorShape([Dimension(None)]))\n"
>>>>>>> refs/remotes/origin/himanshu
     ]
    }
   ],
   "source": [
    "test_label_hot = onehot_labels(test_rep)\n",
    "train_label_hot = onehot_labels(train_rep)\n",
    "\n",
<<<<<<< HEAD
    "print(train_label_hot.shape, test_label_hot.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# LOGDIR = '/home/physics/btech/ph1140797/AHS-ML-Project/tf_logs'\n",
    "\n",
    "# def hidden_layer(input, size_in, size_out, name='hidden'):\n",
    "#     with tf.name_scope(name):\n",
    "#         w = tf.Variable(tf.truncated_normal([size_in, size_out], stddev=0.1), name=\"W\")\n",
    "#         b = tf.Variable(tf.constant(0.1, shape=[size_out]), name=\"B\")\n",
    "#         act = tf.nn.relu(tf.matmul(input, w) + b)\n",
=======
    "def hidden_layer(input, size_in, size_out, name='hidden'):\n",
    "    with tf.name_scope(name):\n",
    "        w = tf.Variable(tf.truncated_normal([size_in, size_out], stddev=0.1), name=\"W\")\n",
    "        b = tf.Variable(tf.constant(0.1, shape=[size_out]), name=\"B\")\n",
    "        act = tf.nn.relu(tf.matmul(input, w) + b)\n",
>>>>>>> refs/remotes/origin/himanshu
    "        \n",
    "#         tf.summary.histogram(\"weights\", w)\n",
    "#         tf.summary.histogram(\"biases\", b)\n",
    "#         tf.summary.histogram(\"activations\", act)\n",
<<<<<<< HEAD
    "#         return act\n",
    "\n",
    "# graph_ahs = tf.Graph()\n",
    "\n",
    "# with graph_ahs.as_default():\n",
    "#     SIZE = train_data.shape[1]\n",
    "#     HOT = train_label_hot.shape[1]\n",
    "\n",
    "#     X = tf.placeholder(tf.float32, [None, SIZE], name='X')\n",
    "#     Y_ = tf.placeholder(tf.float32, [None, HOT], name='Labels')\n",
    "    \n",
    "#     # Learning Rate - alpha\n",
    "#     alpha = tf.placeholder(tf.float32)\n",
    "#     # Dropout Probablity\n",
    "#     pkeep = tf.placeholder(tf.float32)\n",
    "    \n",
    "#     K = 100    # Neurons in 1st hidden layer\n",
    "#     L = 75     # Neurons in 2nd hidden layer\n",
    "#     M = 50     # Neurons in 3rd hidden layer\n",
    "    \n",
    "#     Y1 = hidden_layer(X, SIZE, K, name='hidden-1')\n",
    "#     Y2 = hidden_layer(Y1, K, L, name='hidden-2')\n",
    "#     Y3 = hidden_layer(Y2, L, M, name='hidden-3')\n",
    "#     Y_drop = tf.nn.dropout(Y3, pkeep, name='dropout')\n",
    "#     YF = hidden_layer(Y_drop, M, HOT, name='output')\n",
    "    \n",
    "#     Y_f = tf.nn.softmax(YF)\n",
    "    \n",
    "#     print('Y1 : ',Y1.get_shape())\n",
    "#     print('Y2 : ',Y2.get_shape())\n",
    "#     print('Y3 : ',Y3.get_shape())\n",
    "#     print('YF : ',YF.get_shape())\n",
    "        \n",
    "#     with tf.name_scope('Cross_Entropy'):\n",
    "#         cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=Y_f, labels=Y_)) \n",
    "#         tf.summary.scalar('Cross_Entropy', cross_entropy)\n",
    "    \n",
    "#     with tf.name_scope('Train'):\n",
    "#         train_step = tf.train.AdamOptimizer(alpha).minimize(cross_entropy)\n",
    "    \n",
    "#     with tf.name_scope('Accuracy'):\n",
    "#         correct_prediction = tf.equal(tf.argmax(Y_f,1), tf.argmax(Y_,1))\n",
    "#         accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "#         tf.summary.scalar('Accuracy', accuracy)\n",
    "    \n",
    "#     summary = tf.summary.merge_all()\n",
    "#     model_saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Y1 : ', TensorShape([Dimension(None), Dimension(300)]))\n",
      "('Y2 : ', TensorShape([Dimension(None), Dimension(200)]))\n",
      "('Y3 : ', TensorShape([Dimension(None), Dimension(300)]))\n",
      "('YF : ', TensorShape([Dimension(None), Dimension(9)]))\n"
     ]
    }
   ],
   "source": [
    "LOGDIR = '/home/physics/btech/ph1140797/AHS-ML-Project/tf_logs'\n",
    "\n",
    "def hidden_layer(input, size_in, size_out, tst, name='hidden', drop=True, iter=1):\n",
    "    with tf.name_scope(name):\n",
    "        w = tf.get_variable(name+\"W\", [size_in, size_out], initializer = tf.contrib.layers.xavier_initializer())\n",
    "        # w = tf.Variable(tf.truncated_normal([size_in, size_out], stddev=0.1), name=\"W\")\n",
    "        b = tf.Variable(tf.constant(0.1, shape=[size_out]), name=\"B\")\n",
    "        y = tf.matmul(input, w)\n",
    "        Y_bn, update_ema = batchnorm(y, tst, iter, b)\n",
    "        act = tf.nn.relu(Y_bn)\n",
    "        \n",
    "        if drop:\n",
    "            act = tf.nn.dropout(act, 0.50)\n",
    "        \n",
    "        tf.summary.histogram(\"weights\", w)\n",
    "        tf.summary.histogram(\"biases\", b)\n",
    "        tf.summary.histogram(\"activations\", act)\n",
    "        return act\n",
    "    \n",
    "def batchnorm(Ylogits, is_test, iteration, offset):\n",
    "    exp_moving_avg = tf.train.ExponentialMovingAverage(0.999, iteration) # adding the iteration prevents from averaging across non-existing iterations\n",
    "    bnepsilon = 1e-5\n",
    "    \n",
    "    mean, variance = tf.nn.moments(Ylogits, [0])\n",
    "    update_moving_everages = exp_moving_avg.apply([mean, variance])\n",
    "    m = tf.cond(is_test, lambda: exp_moving_avg.average(mean), lambda: mean)\n",
    "    v = tf.cond(is_test, lambda: exp_moving_avg.average(variance), lambda: variance)\n",
    "    Ybn = tf.nn.batch_normalization(Ylogits, m, v, offset, None, bnepsilon)\n",
    "    return Ybn, update_moving_everages\n",
    "\n",
    "graph_ahs = tf.Graph()\n",
    "\n",
    "with graph_ahs.as_default():\n",
    "    SIZE = train_data.shape[1]\n",
    "    HOT = train_label_hot.shape[1]\n",
    "\n",
    "    X = tf.placeholder(tf.float32, [None, SIZE], name='X')\n",
    "    Y_ = tf.placeholder(tf.float32, [None, HOT], name='Labels')\n",
    "    \n",
    "    # Learning Rate - alpha\n",
    "    alpha = tf.placeholder(tf.float32)\n",
    "    # Dropout Probablity\n",
    "    pkeep = tf.placeholder(tf.float32)\n",
    "    \n",
    "    iter = tf.placeholder(tf.int32)\n",
    "    tst = tf.placeholder(tf.bool)\n",
    "    \n",
    "    W_in = tf.get_variable(\"W\", [SIZE, 200], initializer = tf.contrib.layers.xavier_initializer())\n",
    "    b_in = tf.Variable(tf.constant(0.1, shape=[200]), name=\"B\")\n",
    "    Y_in = tf.nn.relu(tf.matmul(X, W_in) + b_in)\n",
    "    \n",
    "    Y1 = hidden_layer(Y_in, 200, 300, tst, name='hidden-1', iter=iter)\n",
    "    Y2 = hidden_layer(Y1, 300, 200, tst, name='hidden-2', iter=iter)\n",
    "    Y3 = hidden_layer(Y2, 200, 300, tst, name='hidden-3', iter=iter)\n",
    "    Y4 = hidden_layer(Y3, 300, 200, tst, name='hidden-4', iter=iter)\n",
    "    Y5 = hidden_layer(Y4, 200, 300, tst, name='hidden-5', iter=iter)\n",
    "    Y6 = hidden_layer(Y5, 300, 200, tst, name='hidden-6', iter=iter)\n",
    "    Y7 = hidden_layer(Y6, 200, 300, tst, name='hidden-7', iter=iter)\n",
    "    Y8 = hidden_layer(Y7, 300, 200, tst, name='hidden-8', iter=iter)\n",
    "    Y9 = hidden_layer(Y8, 200, 300, tst, name='hidden-9', iter=iter)\n",
    "    Y10 = hidden_layer(Y9, 300, 200, tst, name='hidden-10', iter=iter)\n",
    "    Y11 = hidden_layer(Y10, 200, 300, tst, name='hidden-11', iter=iter)\n",
    "    Y12 = hidden_layer(Y11, 300, 200, tst, name='hidden-12', iter=iter)\n",
    "    Y13 = hidden_layer(Y12, 200, 300, tst, name='hidden-13', iter=iter)\n",
    "    Y14 = hidden_layer(Y13, 300, 200, tst, name='hidden-14', iter=iter)\n",
    "    Y15 = hidden_layer(Y14, 200, 300, tst, name='hidden-15', iter=iter)\n",
    "    Y16 = hidden_layer(Y15, 300, 200, tst, name='hidden-16', iter=iter)\n",
    "    Y17 = hidden_layer(Y16, 200, 300, tst, name='hidden-17', iter=iter)\n",
    "    Y18 = hidden_layer(Y17, 300, 200, tst, name='hidden-18', iter=iter)\n",
    "    Y19 = hidden_layer(Y18, 200, 300, tst, name='hidden-19', iter=iter)\n",
    "    Y20 = hidden_layer(Y19, 300, 200, tst, name='hidden-20', iter=iter)\n",
    "    \n",
    "    YF = hidden_layer(Y20, 200, HOT, tst, name='output', drop=False)\n",
    "    \n",
    "    Y_f = tf.nn.softmax(YF)\n",
    "    \n",
    "    print('Y1 : ',Y1.get_shape())\n",
    "    print('Y2 : ',Y2.get_shape())\n",
    "    print('Y3 : ',Y3.get_shape())\n",
    "    print('YF : ',YF.get_shape())\n",
    "        \n",
    "    with tf.name_scope('Cross_Entropy'):\n",
    "        cross_entropy =  tf.reduce_mean(-tf.reduce_sum(Y_ * tf.log(tf.clip_by_value(Y_f, 1e-8, 1.0)), reduction_indices=[1]))\n",
    "        # cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=Y_f, labels=Y_)) \n",
    "        tf.summary.scalar('Cross_Entropy', cross_entropy)\n",
    "    \n",
    "    with tf.name_scope('Train'):\n",
    "        train_step = tf.train.AdamOptimizer(.00006).minimize(cross_entropy)\n",
    "    \n",
    "    with tf.name_scope('Accuracy'):\n",
    "        correct_prediction = tf.equal(tf.argmax(Y_f,1), tf.argmax(Y_,1))\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "        tf.summary.scalar('Accuracy', accuracy)\n",
    "    \n",
    "    mistakes = tf.not_equal(tf.argmax(Y_, 1), tf.argmax(Y_f, 1))\n",
    "    error = tf.reduce_mean(tf.cast(mistakes, tf.float32))\n",
    "    \n",
    "    summary = tf.summary.merge_all()\n",
=======
    "        \n",
    "        return act\n",
    "    \n",
    "with graph.as_default():\n",
    "    SIZE = train_data.shape[1]\n",
    "    HOT = train_label_hot.shape[1]\n",
    "\n",
    "    X = tf.placeholder(tf.float32, [None, SIZE])\n",
    "    Y_ = tf.placeholder(tf.float32, [None, HOT])\n",
    "    \n",
    "    # Learning Rate - alpha\n",
    "    alpha = tf.placeholder(tf.float32)\n",
    "    # Dropout Probablity\n",
    "    pkeep = tf.placeholder(tf.float32)\n",
    "    \n",
    "    K = 100    # Neurons in 1st hidden layer\n",
    "    L = 75     # Neurons in 2nd hidden layer\n",
    "    M = 50     # Neurons in 3rd hidden layer\n",
    "    \n",
    "    Y1 = hidden_layer(X, SIZE, K, name='hidden-1')\n",
    "    Y2 = hidden_layer(Y1, K, L, name='hidden-2')\n",
    "    Y3 = hidden_layer(Y2, L, M, name='hidden-3')\n",
    "    Y_drop = tf.nn.dropout(Y3, pkeep)\n",
    "    YF = hidden_layer(Y_drop, M, HOT, name='output')\n",
    "    \n",
    "#     W1 = tf.Variable(tf.truncated_normal([SIZE, K], stddev=0.1), name=\"W1\")\n",
    "#     B1 = tf.Variable(tf.constant(0.1, dtype=tf.float32, shape=[K]), name=\"B1\")\n",
    "    \n",
    "#     W2 = tf.Variable(tf.truncated_normal([K, L], stddev=0.1), name=\"W2\")\n",
    "#     B2 = tf.Variable(tf.constant(0.1, dtype=tf.float32, shape=[L]), name=\"B2\")\n",
    "    \n",
    "#     W3 = tf.Variable(tf.truncated_normal([L, M], stddev=0.1), name=\"W3\")\n",
    "#     B3 = tf.Variable(tf.constant(0.1, dtype=tf.float32, shape=[M]), name=\"B3\")\n",
    "    \n",
    "#     WF = tf.Variable(tf.truncated_normal([M, HOT], stddev=0.1), name=\"WF\")\n",
    "#     BF = tf.Variable(tf.constant(0.1, dtype=tf.float32, shape=[HOT]), name=\"BF\")\n",
    "    \n",
    "#     print(W1.get_shape(), W2.get_shape())\n",
    "#     print(W3.get_shape(), WF.get_shape())\n",
    "#     print(B1.get_shape(), B2.get_shape())\n",
    "#     print(B3.get_shape(), BF.get_shape()) \n",
    "    \n",
    "#     # Model\n",
    "#     Y1 = tf.nn.relu(tf.matmul(X, W1) + B1)      # Shape -> \n",
    "#     Y2 = tf.nn.relu(tf.matmul(Y1, W2) + B2)     # Shape -> \n",
    "#     Y3 = tf.nn.relu(tf.matmul(Y2, W3) + B3)     # Shape ->\n",
    "#     YF = tf.nn.relu(tf.matmul(tf.nn.dropout(Y3, pkeep), WF) + BF)     # Shape ->\n",
    "    \n",
    "    print('Y1 : ',Y1.get_shape())\n",
    "    print('Y2 : ',Y2.get_shape())\n",
    "    print('Y3 : ',Y3.get_shape())\n",
    "    print('YF : ',YF.get_shape())\n",
    "    \n",
    "    Y_f = tf.nn.softmax(YF)\n",
    "    \n",
    "    YY = tf.nn.softmax_cross_entropy_with_logits(logits=Y_f, labels=Y_)\n",
    "#     YY = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=Y_f, labels=Y_)\n",
    "    \n",
    "#     YY = -tf.reduce_sum(Y_ * tf.log(Y_f), [1])\n",
    "    \n",
    "    cross_entropy = tf.reduce_mean(YY)\n",
    "    \n",
    "    print('Yf : ',Y_f.get_shape())\n",
    "    print('YY : ',YY.get_shape())\n",
    "    \n",
    "#     cross_entropy = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=YF, labels=Y_))\n",
    "    \n",
    "    \n",
    "    train_step = tf.train.AdamOptimizer(alpha).minimize(cross_entropy)\n",
    "\n",
    "    train_prediction = tf.pack([YF, Y_f])\n",
    "    YYY = tf.pack([YY])\n",
    "    yy_ = tf.pack([Y_])\n",
    "    \n",
    "    correct_prediction = tf.equal(tf.argmax(Y_f,1), tf.argmax(Y_,1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    \n",
    "    W_s = tf.pack([tf.reduce_max(tf.abs(W1)),tf.reduce_max(tf.abs(W2)),tf.reduce_max(tf.abs(W3))\\\n",
    "                   ,tf.reduce_max(tf.abs(WF))])\n",
    "    b_s = tf.pack([tf.reduce_max(tf.abs(B1)),tf.reduce_max(tf.abs(B2)),tf.reduce_max(tf.abs(B3))\\\n",
    "                   ,tf.reduce_max(tf.abs(BF))])\n",
    "    \n",
>>>>>>> refs/remotes/origin/himanshu
    "    model_saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
=======
   "execution_count": 145,
   "metadata": {
    "collapsed": false,
    "scrolled": true
>>>>>>> refs/remotes/origin/himanshu
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "('train : ', (134330, 303), '  label : ', (134330, 9))\n",
      "Initialized\n",
      "('Epoch : ', 1)\n",
      "Loss at step 0: 2.402847\n",
      "Minibatch accuracy: 13.000%\n",
      "    \n",
      "Loss at step 500: 2.337563\n",
      "Minibatch accuracy: 12.000%\n",
      "    \n",
      "Loss at step 1000: 2.327590\n",
      "Minibatch accuracy: 12.000%\n",
      "    \n",
      "-----------------\n",
      "('training error is ', 0.12531079)\n",
      "('Epoch : ', 2)\n",
      "Loss at step 0: 2.322671\n",
      "Minibatch accuracy: 8.000%\n",
      "    \n",
      "Loss at step 500: 2.271604\n",
      "Minibatch accuracy: 9.000%\n",
      "    \n",
      "Loss at step 1000: 2.226622\n",
      "Minibatch accuracy: 7.000%\n",
      "    \n",
      "-----------------\n",
      "('training error is ', 0.22748451)\n",
      "('Epoch : ', 3)\n",
      "Loss at step 0: 2.228990\n",
      "Minibatch accuracy: 13.000%\n",
      "    \n",
      "Loss at step 500: 2.203989\n",
      "Minibatch accuracy: 16.000%\n",
      "    \n",
      "Loss at step 1000: 2.165712\n",
      "Minibatch accuracy: 25.000%\n",
      "    \n",
      "-----------------\n",
      "('training error is ', 0.22784933)\n",
      "('Epoch : ', 4)\n",
      "Loss at step 0: 2.152376\n",
      "Minibatch accuracy: 18.000%\n",
      "    \n",
      "Loss at step 500: 2.166884\n",
      "Minibatch accuracy: 17.000%\n",
      "    \n",
      "Loss at step 1000: 2.178901\n",
      "Minibatch accuracy: 24.000%\n",
      "    \n",
      "-----------------\n",
      "('training error is ', 0.2278344)\n",
      "('Epoch : ', 5)\n",
      "Loss at step 0: 2.135662\n",
      "Minibatch accuracy: 20.000%\n",
      "    \n",
      "Loss at step 500: 2.124534\n",
      "Minibatch accuracy: 20.000%\n",
      "    \n",
      "Loss at step 1000: 2.151051\n",
      "Minibatch accuracy: 23.000%\n",
      "    \n",
      "-----------------\n",
      "('training error is ', 0.22783442)\n",
      "('Epoch : ', 6)\n",
      "Loss at step 0: 2.122403\n",
      "Minibatch accuracy: 22.000%\n",
      "    \n",
      "Loss at step 500: 2.128096\n",
      "Minibatch accuracy: 21.000%\n",
      "    \n",
      "Loss at step 1000: 2.135032\n",
      "Minibatch accuracy: 25.000%\n",
      "    \n",
      "-----------------\n",
      "('training error is ', 0.2278344)\n",
      "('Epoch : ', 7)\n",
      "Loss at step 0: 2.083977\n",
      "Minibatch accuracy: 26.000%\n",
      "    \n",
      "Loss at step 500: 2.124057\n",
      "Minibatch accuracy: 20.000%\n",
      "    \n",
      "Loss at step 1000: 2.150762\n",
      "Minibatch accuracy: 24.000%\n",
      "    \n",
      "-----------------\n",
      "('training error is ', 0.2278344)\n",
      "('Epoch : ', 8)\n",
      "Loss at step 0: 2.057321\n",
      "Minibatch accuracy: 26.000%\n",
      "    \n",
      "Loss at step 500: 2.126223\n",
      "Minibatch accuracy: 21.000%\n",
      "    \n",
      "Loss at step 1000: 2.140544\n",
      "Minibatch accuracy: 24.000%\n",
      "    \n",
      "-----------------\n",
      "('training error is ', 0.22781208)\n",
      "('Epoch : ', 9)\n",
      "Loss at step 0: 2.067252\n",
      "Minibatch accuracy: 25.000%\n",
      "    \n",
      "Loss at step 500: 2.095815\n",
      "Minibatch accuracy: 21.000%\n",
      "    \n",
      "Loss at step 1000: 2.140790\n",
      "Minibatch accuracy: 23.000%\n",
      "    \n",
      "-----------------\n",
      "('training error is ', 0.22655401)\n",
      "('Epoch : ', 10)\n",
      "Loss at step 0: 2.050131\n",
      "Minibatch accuracy: 25.000%\n",
      "    \n",
      "Loss at step 500: 2.142946\n",
      "Minibatch accuracy: 15.000%\n",
      "    \n",
      "Loss at step 1000: 2.137512\n",
      "Minibatch accuracy: 17.000%\n",
      "    \n",
      "-----------------\n",
      "('training error is ', 0.2177771)\n",
      "('Epoch : ', 11)\n",
      "Loss at step 0: 2.056490\n",
      "Minibatch accuracy: 25.000%\n",
      "    \n",
      "Loss at step 500: 2.123024\n",
      "Minibatch accuracy: 19.000%\n",
      "    \n",
      "Loss at step 1000: 2.110219\n",
      "Minibatch accuracy: 24.000%\n",
      "    \n",
      "-----------------\n",
      "('training error is ', 0.21840242)\n",
      "('Epoch : ', 12)\n",
      "Loss at step 0: 2.045615\n",
      "Minibatch accuracy: 20.000%\n",
      "    \n",
      "Loss at step 500: 2.113682\n",
      "Minibatch accuracy: 18.000%\n",
      "    \n",
      "Loss at step 1000: 2.144930\n",
      "Minibatch accuracy: 20.000%\n",
      "    \n",
      "-----------------\n",
      "('training error is ', 0.21829818)\n",
      "('Epoch : ', 13)\n",
      "Loss at step 0: 2.011941\n",
      "Minibatch accuracy: 23.000%\n",
      "    \n",
      "Loss at step 500: 2.105434\n",
      "Minibatch accuracy: 14.000%\n",
      "    \n",
      "Loss at step 1000: 2.096539\n",
      "Minibatch accuracy: 23.000%\n",
      "    \n",
      "-----------------\n",
      "('training error is ', 0.22250426)\n",
      "('Epoch : ', 14)\n",
      "Loss at step 0: 2.046716\n",
      "Minibatch accuracy: 18.000%\n",
      "    \n",
      "Loss at step 500: 2.086353\n",
      "Minibatch accuracy: 14.000%\n",
      "    \n",
      "Loss at step 1000: 2.088150\n",
      "Minibatch accuracy: 24.000%\n",
      "    \n",
      "-----------------\n",
      "('training error is ', 0.22359119)\n",
      "('Epoch : ', 15)\n",
      "Loss at step 0: 2.034282\n",
      "Minibatch accuracy: 23.000%\n",
      "    \n"
=======
      "('train : ', (134330, 303), '  test : ', (134330, 2))\n",
      "Initialized\n",
      "Loss at step 0: 0.699335\n",
      "Minibatch accuracy: 64.844%\n",
      "    \n",
      "Loss at step 500: 0.453887\n",
      "Minibatch accuracy: 85.938%\n",
      "    \n",
      "Loss at step 1000: 0.508574\n",
      "Minibatch accuracy: 80.469%\n",
      "    \n",
      "Loss at step 0: 0.524938\n",
      "Minibatch accuracy: 80.469%\n",
      "    \n",
      "Loss at step 500: 0.453887\n",
      "Minibatch accuracy: 85.938%\n",
      "    \n",
      "Loss at step 1000: 0.508574\n",
      "Minibatch accuracy: 80.469%\n",
      "    \n",
      "Loss at step 0: 0.511901\n",
      "Minibatch accuracy: 80.469%\n",
      "    \n",
      "Loss at step 500: 0.453887\n",
      "Minibatch accuracy: 85.938%\n",
      "    \n",
      "Loss at step 1000: 0.508574\n",
      "Minibatch accuracy: 80.469%\n",
      "    \n",
      "Loss at step 0: 0.509427\n",
      "Minibatch accuracy: 80.469%\n",
      "    \n",
      "Loss at step 500: 0.453887\n",
      "Minibatch accuracy: 85.938%\n",
      "    \n",
      "Loss at step 1000: 0.508574\n",
      "Minibatch accuracy: 80.469%\n",
      "    \n",
      "Loss at step 0: 0.508674\n",
      "Minibatch accuracy: 80.469%\n",
      "    \n",
      "Loss at step 500: 0.453887\n",
      "Minibatch accuracy: 85.938%\n",
      "    \n",
      "Loss at step 1000: 0.508574\n",
      "Minibatch accuracy: 80.469%\n",
      "    \n",
      "Loss at step 0: 0.508635\n",
      "Minibatch accuracy: 80.469%\n",
      "    \n",
      "Loss at step 500: 0.453887\n",
      "Minibatch accuracy: 85.938%\n",
      "    \n",
      "Loss at step 1000: 0.508574\n",
      "Minibatch accuracy: 80.469%\n",
      "    \n",
      "Loss at step 0: 0.508585\n",
      "Minibatch accuracy: 80.469%\n",
      "    \n",
      "Loss at step 500: 0.453887\n",
      "Minibatch accuracy: 85.938%\n",
      "    \n",
      "Loss at step 1000: 0.508574\n",
      "Minibatch accuracy: 80.469%\n",
      "    \n",
      "Loss at step 0: 0.508612\n",
      "Minibatch accuracy: 80.469%\n",
      "    \n",
      "Loss at step 500: 0.453887\n",
      "Minibatch accuracy: 85.938%\n",
      "    \n",
      "Loss at step 1000: 0.508574\n",
      "Minibatch accuracy: 80.469%\n",
      "    \n",
      "Loss at step 0: 0.508581\n",
      "Minibatch accuracy: 80.469%\n",
      "    \n",
      "Loss at step 500: 0.453887\n",
      "Minibatch accuracy: 85.938%\n",
      "    \n",
      "Loss at step 1000: 0.508574\n",
      "Minibatch accuracy: 80.469%\n",
      "    \n",
      "Loss at step 0: 0.508576\n",
      "Minibatch accuracy: 80.469%\n",
      "    \n",
      "Loss at step 500: 0.453887\n",
      "Minibatch accuracy: 85.938%\n",
      "    \n",
      "Loss at step 1000: 0.508574\n",
      "Minibatch accuracy: 80.469%\n",
      "    \n",
      "Loss at step 0: 0.508576\n",
      "Minibatch accuracy: 80.469%\n",
      "    \n",
      "Loss at step 500: 0.453887\n",
      "Minibatch accuracy: 85.938%\n",
      "    \n",
      "Loss at step 1000: 0.508574\n",
      "Minibatch accuracy: 80.469%\n",
      "    \n",
      "Loss at step 0: 0.508575\n",
      "Minibatch accuracy: 80.469%\n",
      "    \n",
      "Loss at step 500: 0.453887\n",
      "Minibatch accuracy: 85.938%\n",
      "    \n",
      "Loss at step 1000: 0.508574\n",
      "Minibatch accuracy: 80.469%\n",
      "    \n",
      "Loss at step 0: 0.508574\n",
      "Minibatch accuracy: 80.469%\n",
      "    \n",
      "Loss at step 500: 0.453887\n",
      "Minibatch accuracy: 85.938%\n",
      "    \n",
      "Loss at step 1000: 0.508574\n",
      "Minibatch accuracy: 80.469%\n",
      "    \n",
      "Loss at step 0: 0.508574\n",
      "Minibatch accuracy: 80.469%\n",
      "    \n",
      "Loss at step 500: 0.453887\n",
      "Minibatch accuracy: 85.938%\n",
      "    \n",
      "Loss at step 1000: 0.508574\n",
      "Minibatch accuracy: 80.469%\n",
      "    \n",
      "Loss at step 0: 0.508574\n",
      "Minibatch accuracy: 80.469%\n",
      "    \n",
      "Loss at step 500: 0.453887\n",
      "Minibatch accuracy: 85.938%\n",
      "    \n",
      "Loss at step 1000: 0.508574\n",
      "Minibatch accuracy: 80.469%\n",
      "    \n",
      "Loss at step 0: 0.508574\n",
      "Minibatch accuracy: 80.469%\n",
      "    \n",
      "Loss at step 500: 0.453887\n",
      "Minibatch accuracy: 85.938%\n",
      "    \n",
      "Loss at step 1000: 0.508574\n",
      "Minibatch accuracy: 80.469%\n",
      "    \n",
      "Loss at step 0: 0.508574\n",
      "Minibatch accuracy: 80.469%\n",
      "    \n",
      "Loss at step 500: 0.453887\n",
      "Minibatch accuracy: 85.938%\n",
      "    \n",
      "Loss at step 1000: 0.508574\n",
      "Minibatch accuracy: 80.469%\n",
      "    \n",
      "Loss at step 0: 0.508574\n",
      "Minibatch accuracy: 80.469%\n",
      "    \n",
      "Loss at step 500: 0.453887\n",
      "Minibatch accuracy: 85.938%\n",
      "    \n",
      "Loss at step 1000: 0.508574\n",
      "Minibatch accuracy: 80.469%\n",
      "    \n",
      "Loss at step 0: 0.508574\n",
      "Minibatch accuracy: 80.469%\n",
      "    \n",
      "Loss at step 500: 0.453887\n",
      "Minibatch accuracy: 85.938%\n",
      "    \n",
      "Loss at step 1000: 0.508574\n",
      "Minibatch accuracy: 80.469%\n",
      "    \n",
      "Loss at step 0: 0.508574\n",
      "Minibatch accuracy: 80.469%\n",
      "    \n",
      "Loss at step 500: 0.453887\n",
      "Minibatch accuracy: 85.938%\n",
      "    \n",
      "Loss at step 1000: 0.508574\n",
      "Minibatch accuracy: 80.469%\n",
      "    \n",
      "Loss at step 0: 0.508574\n",
      "Minibatch accuracy: 80.469%\n",
      "    \n",
      "Loss at step 500: 0.453887\n",
      "Minibatch accuracy: 85.938%\n",
      "    \n",
      "Loss at step 1000: 0.508574\n",
      "Minibatch accuracy: 80.469%\n",
      "    \n",
      "Loss at step 0: 0.508574\n",
      "Minibatch accuracy: 80.469%\n",
      "    \n",
      "Loss at step 500: 0.453887\n",
      "Minibatch accuracy: 85.938%\n",
      "    \n",
      "Loss at step 1000: 0.508574\n",
      "Minibatch accuracy: 80.469%\n",
      "    \n",
      "Loss at step 0: 0.508574\n",
      "Minibatch accuracy: 80.469%\n",
      "    \n",
      "Loss at step 500: 0.453887\n",
      "Minibatch accuracy: 85.938%\n",
      "    \n",
      "Loss at step 1000: 0.508574\n",
      "Minibatch accuracy: 80.469%\n",
      "    \n",
      "Loss at step 0: 0.508574\n",
      "Minibatch accuracy: 80.469%\n",
      "    \n",
      "Loss at step 500: 0.453887\n",
      "Minibatch accuracy: 85.938%\n",
      "    \n",
      "Loss at step 1000: 0.508574\n",
      "Minibatch accuracy: 80.469%\n",
      "    \n",
      "Training Complete on MNIST Data\n"
>>>>>>> refs/remotes/origin/himanshu
     ]
    },
    {
     "ename": "NotFoundError",
     "evalue": "/models/1_label/label_1.ckpt.tempstate10509209658742328678\n\t [[Node: save/save = SaveSlices[T=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/cpu:0\"](_recv_save/Const_0, save/save/tensor_names, save/save/shapes_and_slices, B1/_45, B1/Adam/_47, B1/Adam_1/_49, B2/_51, B2/Adam/_53, B2/Adam_1/_55, B3/_57, B3/Adam/_59, B3/Adam_1/_61, BF/_63, BF/Adam/_65, BF/Adam_1/_67, W1/_69, W1/Adam/_71, W1/Adam_1/_73, W2/_75, W2/Adam/_77, W2/Adam_1/_79, W3/_81, W3/Adam/_83, W3/Adam_1/_85, WF/_87, WF/Adam/_89, WF/Adam_1/_91, beta1_power/_93, beta2_power/_95)]]\nCaused by op u'save/save', defined at:\n  File \"/home/apps/TENSOR_GPU/lib/python2.7/runpy.py\", line 174, in _run_module_as_main\n    \"__main__\", fname, loader, pkg_name)\n  File \"/home/apps/TENSOR_GPU/lib/python2.7/runpy.py\", line 72, in _run_code\n    exec code in run_globals\n  File \"/home/apps/TENSOR_GPU/lib/python2.7/site-packages/ipykernel/__main__.py\", line 3, in <module>\n    app.launch_new_instance()\n  File \"/home/apps/TENSOR_GPU/lib/python2.7/site-packages/traitlets/config/application.py\", line 596, in launch_instance\n    app.start()\n  File \"/home/apps/TENSOR_GPU/lib/python2.7/site-packages/ipykernel/kernelapp.py\", line 442, in start\n    ioloop.IOLoop.instance().start()\n  File \"/home/apps/TENSOR_GPU/lib/python2.7/site-packages/zmq/eventloop/ioloop.py\", line 162, in start\n    super(ZMQIOLoop, self).start()\n  File \"/home/apps/TENSOR_GPU/lib/python2.7/site-packages/tornado/ioloop.py\", line 883, in start\n    handler_func(fd_obj, events)\n  File \"/home/apps/TENSOR_GPU/lib/python2.7/site-packages/tornado/stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/apps/TENSOR_GPU/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/home/apps/TENSOR_GPU/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/home/apps/TENSOR_GPU/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/home/apps/TENSOR_GPU/lib/python2.7/site-packages/tornado/stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/apps/TENSOR_GPU/lib/python2.7/site-packages/ipykernel/kernelbase.py\", line 276, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/home/apps/TENSOR_GPU/lib/python2.7/site-packages/ipykernel/kernelbase.py\", line 228, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/home/apps/TENSOR_GPU/lib/python2.7/site-packages/ipykernel/kernelbase.py\", line 391, in execute_request\n    user_expressions, allow_stdin)\n  File \"/home/apps/TENSOR_GPU/lib/python2.7/site-packages/ipykernel/ipkernel.py\", line 199, in do_execute\n    shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/home/apps/TENSOR_GPU/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2723, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/home/apps/TENSOR_GPU/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2825, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/home/apps/TENSOR_GPU/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2885, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-144-b538f1a2b2d6>\", line 87, in <module>\n    model_saver = tf.train.Saver()\n  File \"/home/apps/TENSOR_GPU/lib/python2.7/site-packages/tensorflow/python/training/saver.py\", line 784, in __init__\n    restore_sequentially=restore_sequentially)\n  File \"/home/apps/TENSOR_GPU/lib/python2.7/site-packages/tensorflow/python/training/saver.py\", line 452, in build\n    save_tensor = self._AddSaveOps(filename_tensor, vars_to_save)\n  File \"/home/apps/TENSOR_GPU/lib/python2.7/site-packages/tensorflow/python/training/saver.py\", line 153, in _AddSaveOps\n    save = self.save_op(filename_tensor, vars_to_save)\n  File \"/home/apps/TENSOR_GPU/lib/python2.7/site-packages/tensorflow/python/training/saver.py\", line 105, in save_op\n    tensor_slices=[vs.slice_spec for vs in vars_to_save])\n  File \"/home/apps/TENSOR_GPU/lib/python2.7/site-packages/tensorflow/python/ops/io_ops.py\", line 169, in _save\n    tensors, name=name)\n  File \"/home/apps/TENSOR_GPU/lib/python2.7/site-packages/tensorflow/python/ops/gen_io_ops.py\", line 341, in _save_slices\n    name=name)\n  File \"/home/apps/TENSOR_GPU/lib/python2.7/site-packages/tensorflow/python/ops/op_def_library.py\", line 661, in apply_op\n    op_def=op_def)\n  File \"/home/apps/TENSOR_GPU/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 2040, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/home/apps/TENSOR_GPU/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 1087, in __init__\n    self._traceback = _extract_stack()\n",
     "output_type": "error",
     "traceback": [
<<<<<<< HEAD
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-620c70114f8a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     29\u001b[0m             \u001b[0mfeed_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mX\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mbatch_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mbatch_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpkeep\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0;36m0.50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtst\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miter\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m             \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcross_entropy\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m500\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/apps/TENSORFLOW/1.0/gpu/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    765\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 767\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    768\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    769\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/apps/TENSORFLOW/1.0/gpu/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    963\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    964\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 965\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    966\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/apps/TENSORFLOW/1.0/gpu/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1013\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1014\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1015\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1016\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1017\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/home/apps/TENSORFLOW/1.0/gpu/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1020\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1021\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1022\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1023\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1024\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/apps/TENSORFLOW/1.0/gpu/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1002\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1003\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1004\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1005\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1006\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
=======
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-145-240c4444cf4e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     85\u001b[0m \u001b[1;31m#     print('Test Accuracy : ', mean(test_acc))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     86\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 87\u001b[1;33m     \u001b[0msave_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel_saver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'/models/1_label/label_1.ckpt'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     88\u001b[0m     \u001b[1;32mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Model saved in file: %s\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0msave_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/apps/TENSOR_GPU/lib/python2.7/site-packages/tensorflow/python/training/saver.pyc\u001b[0m in \u001b[0;36msave\u001b[1;34m(self, sess, save_path, global_step, latest_filename, meta_graph_suffix)\u001b[0m\n\u001b[0;32m    960\u001b[0m     model_checkpoint_path = sess.run(\n\u001b[0;32m    961\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msaver_def\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave_tensor_name\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 962\u001b[1;33m         {self.saver_def.filename_tensor_name: checkpoint_file})\n\u001b[0m\u001b[0;32m    963\u001b[0m     \u001b[0mmodel_checkpoint_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_str\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_checkpoint_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    964\u001b[0m     self._MaybeDeleteOldCheckpoints(model_checkpoint_path,\n",
      "\u001b[1;32m/home/apps/TENSOR_GPU/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict)\u001b[0m\n\u001b[0;32m    313\u001b[0m         \u001b[1;33m`\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m`\u001b[0m \u001b[0mthat\u001b[0m \u001b[0mdoesn\u001b[0m\u001b[0;31m'\u001b[0m\u001b[0mt\u001b[0m \u001b[0mexist\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    314\u001b[0m     \"\"\"\n\u001b[1;32m--> 315\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_run\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    316\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    317\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mpartial_run\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/apps/TENSOR_GPU/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict)\u001b[0m\n\u001b[0;32m    509\u001b[0m     \u001b[1;31m# Run request and get response.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    510\u001b[0m     results = self._do_run(handle, target_list, unique_fetches,\n\u001b[1;32m--> 511\u001b[1;33m                            feed_dict_string)\n\u001b[0m\u001b[0;32m    512\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    513\u001b[0m     \u001b[1;31m# User may have fetched the same tensor multiple times, but we\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/apps/TENSOR_GPU/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict)\u001b[0m\n\u001b[0;32m    562\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    563\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[1;32m--> 564\u001b[1;33m                            target_list)\n\u001b[0m\u001b[0;32m    565\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    566\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[1;32m/home/apps/TENSOR_GPU/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m    584\u001b[0m         \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    585\u001b[0m         raise errors._make_specific_exception(node_def, op, error_message,\n\u001b[1;32m--> 586\u001b[1;33m                                               e.code)\n\u001b[0m\u001b[0;32m    587\u001b[0m         \u001b[1;31m# pylint: enable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    588\u001b[0m       \u001b[0msix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0me_value\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0me_traceback\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNotFoundError\u001b[0m: /models/1_label/label_1.ckpt.tempstate10509209658742328678\n\t [[Node: save/save = SaveSlices[T=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/cpu:0\"](_recv_save/Const_0, save/save/tensor_names, save/save/shapes_and_slices, B1/_45, B1/Adam/_47, B1/Adam_1/_49, B2/_51, B2/Adam/_53, B2/Adam_1/_55, B3/_57, B3/Adam/_59, B3/Adam_1/_61, BF/_63, BF/Adam/_65, BF/Adam_1/_67, W1/_69, W1/Adam/_71, W1/Adam_1/_73, W2/_75, W2/Adam/_77, W2/Adam_1/_79, W3/_81, W3/Adam/_83, W3/Adam_1/_85, WF/_87, WF/Adam/_89, WF/Adam_1/_91, beta1_power/_93, beta2_power/_95)]]\nCaused by op u'save/save', defined at:\n  File \"/home/apps/TENSOR_GPU/lib/python2.7/runpy.py\", line 174, in _run_module_as_main\n    \"__main__\", fname, loader, pkg_name)\n  File \"/home/apps/TENSOR_GPU/lib/python2.7/runpy.py\", line 72, in _run_code\n    exec code in run_globals\n  File \"/home/apps/TENSOR_GPU/lib/python2.7/site-packages/ipykernel/__main__.py\", line 3, in <module>\n    app.launch_new_instance()\n  File \"/home/apps/TENSOR_GPU/lib/python2.7/site-packages/traitlets/config/application.py\", line 596, in launch_instance\n    app.start()\n  File \"/home/apps/TENSOR_GPU/lib/python2.7/site-packages/ipykernel/kernelapp.py\", line 442, in start\n    ioloop.IOLoop.instance().start()\n  File \"/home/apps/TENSOR_GPU/lib/python2.7/site-packages/zmq/eventloop/ioloop.py\", line 162, in start\n    super(ZMQIOLoop, self).start()\n  File \"/home/apps/TENSOR_GPU/lib/python2.7/site-packages/tornado/ioloop.py\", line 883, in start\n    handler_func(fd_obj, events)\n  File \"/home/apps/TENSOR_GPU/lib/python2.7/site-packages/tornado/stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/apps/TENSOR_GPU/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/home/apps/TENSOR_GPU/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/home/apps/TENSOR_GPU/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/home/apps/TENSOR_GPU/lib/python2.7/site-packages/tornado/stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/apps/TENSOR_GPU/lib/python2.7/site-packages/ipykernel/kernelbase.py\", line 276, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/home/apps/TENSOR_GPU/lib/python2.7/site-packages/ipykernel/kernelbase.py\", line 228, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/home/apps/TENSOR_GPU/lib/python2.7/site-packages/ipykernel/kernelbase.py\", line 391, in execute_request\n    user_expressions, allow_stdin)\n  File \"/home/apps/TENSOR_GPU/lib/python2.7/site-packages/ipykernel/ipkernel.py\", line 199, in do_execute\n    shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/home/apps/TENSOR_GPU/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2723, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/home/apps/TENSOR_GPU/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2825, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/home/apps/TENSOR_GPU/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2885, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-144-b538f1a2b2d6>\", line 87, in <module>\n    model_saver = tf.train.Saver()\n  File \"/home/apps/TENSOR_GPU/lib/python2.7/site-packages/tensorflow/python/training/saver.py\", line 784, in __init__\n    restore_sequentially=restore_sequentially)\n  File \"/home/apps/TENSOR_GPU/lib/python2.7/site-packages/tensorflow/python/training/saver.py\", line 452, in build\n    save_tensor = self._AddSaveOps(filename_tensor, vars_to_save)\n  File \"/home/apps/TENSOR_GPU/lib/python2.7/site-packages/tensorflow/python/training/saver.py\", line 153, in _AddSaveOps\n    save = self.save_op(filename_tensor, vars_to_save)\n  File \"/home/apps/TENSOR_GPU/lib/python2.7/site-packages/tensorflow/python/training/saver.py\", line 105, in save_op\n    tensor_slices=[vs.slice_spec for vs in vars_to_save])\n  File \"/home/apps/TENSOR_GPU/lib/python2.7/site-packages/tensorflow/python/ops/io_ops.py\", line 169, in _save\n    tensors, name=name)\n  File \"/home/apps/TENSOR_GPU/lib/python2.7/site-packages/tensorflow/python/ops/gen_io_ops.py\", line 341, in _save_slices\n    name=name)\n  File \"/home/apps/TENSOR_GPU/lib/python2.7/site-packages/tensorflow/python/ops/op_def_library.py\", line 661, in apply_op\n    op_def=op_def)\n  File \"/home/apps/TENSOR_GPU/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 2040, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/home/apps/TENSOR_GPU/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 1087, in __init__\n    self._traceback = _extract_stack()\n"
>>>>>>> refs/remotes/origin/himanshu
     ]
    }
   ],
   "source": [
    "train_data = train_data\n",
<<<<<<< HEAD
    "label_data = train_label_hot\n",
    "print('train : ', train_data.shape, '  label : ', label_data.shape)\n",
=======
    "label_data = train_label_hot   #[:,np.newaxis]\n",
    "print('train : ', train_data.shape, '  test : ', label_data.shape)\n",
    "\n",
>>>>>>> refs/remotes/origin/himanshu
    "\n",
    "batch_size = 100\n",
    "num_batches = int(label_data.shape[0] / batch_size)\n",
    "num_epochs = 200\n",
    "\n",
    "with tf.Session(graph=graph_ahs) as session:\n",
    "    writer = tf.summary.FileWriter(LOGDIR, graph_ahs)    \n",
    "    tf.global_variables_initializer().run()\n",
    "    print('Initialized')\n",
    "    \n",
    "    for epoch in range(num_epochs - 1):\n",
    "        print('Epoch : ', epoch+1)\n",
    "#         res_epoch = {}\n",
    "#         t_data = svhn_test_box_dataset[epoch*test_batch:(epoch+1)*test_batch]\n",
    "#         t_label = svhn_test_box_labels[epoch*test_batch:(epoch+1)*test_batch]\n",
    "        \n",
    "        for step in range(num_batches - 1):\n",
    "            max_learning_rate = 0.001\n",
    "            min_learning_rate = 0.00001\n",
    "\n",
    "            decay_speed = 5000.0\n",
    "            learning_rate = min_learning_rate + (max_learning_rate - min_learning_rate) * math.exp(-step/decay_speed)\n",
    "\n",
<<<<<<< HEAD
    "            batch_data = train_data[step*batch_size:(step + 1)*batch_size]\n",
    "            batch_labels = label_data[step*batch_size:(step + 1)*batch_size]\n",
    "            feed_dict = {X : batch_data, Y_ : batch_labels, pkeep : 0.50, alpha : learning_rate, tst : False, iter : step}\n",
    "            \n",
    "            session.run([train_step, cross_entropy], feed_dict=feed_dict)\n",
    "            \n",
    "            if (step % 500 == 0):\n",
    "                _, l, accc = session.run([train_step, cross_entropy, accuracy], feed_dict=feed_dict)\n",
    "                print('Loss at step %d: %f' % (step, l))\n",
    "                print('Minibatch accuracy: %.3f%%' % (accc*100))\n",
    "                # print('Minibatch error: %.3f%%' % (error*100))\n",
=======
    "            batch_data = train_data[step*batch_size:(step + 1)*batch_size,:]\n",
    "#             batch_labels = label_data[step*batch_size:(step + 1)*batch_size, :]\n",
    "            batch_labels = label_data[step*batch_size:(step + 1)*batch_size]\n",
    "\n",
    "            feed_dict = {X : batch_data, Y_ : batch_labels, pkeep : 0.80, alpha : learning_rate}\n",
    "            _, l, train_pred, yyy, y_, w, b, accc = session.run([train_step, cross_entropy, train_prediction, YYY, yy_, W_s, b_s, accuracy], feed_dict=feed_dict)\n",
    "            \n",
    "#             accuracy = float(acc(train_pred[0], batch_labels))\n",
    "\n",
    "            if (step % 500 == 0):\n",
    "#                 print('Input : ',y_)\n",
    "#                 print(' ')\n",
    "#                 print('YF - W*X+b : ',train_pred[0])\n",
    "#                 print(' ')\n",
    "#                 print('Y_f- soft(): ',train_pred[1])\n",
    "#                 print(' ')\n",
    "#                 print('YY - CE on soft :',yyy)\n",
    "#                 print(' ')\n",
    "#                 print('Weights : ', w)\n",
    "#                 print('Biases : ', b)\n",
    "#                 minibatch = {}\n",
    "#                 minibatch['loss'] = l\n",
    "#                 minibatch['W'] = W\n",
    "#                 minibatch['B'] = b\n",
    "#                 minibatch['accuracy'] = \"%.2f\" % accuracy\n",
    "\n",
    "#                 res_epoch[int(step/500)] = minibatch\n",
    "                print('Loss at step %d: %f' % (step, l))\n",
    "#                 print('----------------------------')\n",
    "                print('Minibatch accuracy: %.3f%%' % (accc*100))  #(train_pred, batch_labels))\n",
>>>>>>> refs/remotes/origin/himanshu
    "                print('    ')\n",
    "        print('-----------------')\n",
    "        \n",
    "        print (\"training error is \", session.run(accuracy, feed_dict = {X : train_data, Y_ : label_data, pkeep : 0.50, alpha : learning_rate, tst : True, iter : epoch}))\n",
    "            \n",
    "    print('Training Complete on MNIST Data')\n",
<<<<<<< HEAD
    "    \n",
    "    save_path = model_saver.save(session, 'models/1_label/label_1.ckpt')\n",
=======
    "#     print('Test Accuracy : ', mean(test_acc))\n",
    "    \n",
    "    save_path = model_saver.save(session, '/models/1_label/label_1.ckpt')\n",
>>>>>>> refs/remotes/origin/himanshu
    "    print(\"Model saved in file: %s\" % save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data = train_data\n",
    "label_data = train_label_hot\n",
    "print('train : ', train_data.shape, '  label : ', label_data.shape)\n",
    "\n",
    "batch_size = 100\n",
    "num_batches = int(label_data.shape[0] / batch_size)\n",
    "num_epochs = 200\n",
    "\n",
    "with tf.Session(graph=graph_ahs) as session:\n",
    "    writer = tf.summary.FileWriter(LOGDIR, graph_ahs)    \n",
    "    tf.global_variables_initializer().run()\n",
    "    print('Initialized')\n",
    "    \n",
    "    for epoch in range(num_epochs - 1):\n",
    "        print('Epoch : ', epoch+1)\n",
    "#         res_epoch = {}\n",
    "#         t_data = svhn_test_box_dataset[epoch*test_batch:(epoch+1)*test_batch]\n",
    "#         t_label = svhn_test_box_labels[epoch*test_batch:(epoch+1)*test_batch]\n",
    "        \n",
    "        for step in range(num_batches - 1):\n",
    "            max_learning_rate = 0.001\n",
    "            min_learning_rate = 0.00001\n",
    "\n",
    "            decay_speed = 5000.0\n",
    "            learning_rate = min_learning_rate + (max_learning_rate - min_learning_rate) * math.exp(-step/decay_speed)\n",
    "\n",
    "            batch_data = train_data[step*batch_size:(step + 1)*batch_size]\n",
    "            batch_labels = label_data[step*batch_size:(step + 1)*batch_size]\n",
    "            feed_dict = {X : batch_data, Y_ : batch_labels, pkeep : 0.50, alpha : learning_rate}\n",
    "            \n",
    "            session.run([train_step, cross_entropy], feed_dict=feed_dict)\n",
    "            \n",
    "            if (step % 500 == 0):\n",
    "                _, l, accc = session.run([train_step, cross_entropy, accuracy], feed_dict=feed_dict)\n",
    "                print('Loss at step %d: %f' % (step, l))\n",
    "                print('Minibatch accuracy: %.3f%%' % (accc*100))\n",
    "                # print('Minibatch error: %.3f%%' % (error*100))\n",
    "                print('    ')\n",
    "        print('-----------------')\n",
    "        \n",
    "        print (\"training error is \", session.run(accuracy, feed_dict = {X : train_data, Y_ : label_data, pkeep : 0.50, alpha : learning_rate}))\n",
    "            \n",
    "    print('Training Complete on MNIST Data')\n",
    "    \n",
    "    save_path = model_saver.save(session, 'models/1_label/label_1.ckpt')\n",
    "    print(\"Model saved in file: %s\" % save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
<<<<<<< HEAD
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "hidden_dict = {}\n",
    "\n",
    "def hidden_layer(input, size_in, size_out, name='hidden'):\n",
    "    with tf.name_scope(name):\n",
    "#         print(input.get_shape())\n",
    "        w = tf.Variable(tf.truncated_normal([size_in, size_out], stddev=0.1), name=\"W\")\n",
    "#         print(w.get_shape())\n",
    "        b = tf.Variable(tf.constant(0.1, shape=[size_out]), name=\"B\")\n",
    "#         print(b.get_shape())\n",
    "        act = tf.nn.relu(tf.matmul(input, w) + b)\n",
    "        \n",
    "        tf.summary.histogram(\"weights\", w)\n",
    "        tf.summary.histogram(\"biases\", b)\n",
    "        tf.summary.histogram(\"activations\", act)\n",
    "        return act\n",
    "\n",
    "def test_layer_performance(no_of_hidden_layers) :\n",
    "    graph_a = tf.Graph()\n",
    "    with graph_a.as_default():\n",
    "#         train_data = train_data\n",
    "        label_data = train_label_hot\n",
    "\n",
    "        SIZE = train_data.shape[1]\n",
    "        HOT = train_label_hot.shape[1]\n",
    "\n",
    "        X = tf.placeholder(tf.float32, [None, SIZE], name='X')\n",
    "        Y_ = tf.placeholder(tf.float32, [None, HOT], name='Labels')\n",
    "\n",
    "        # Learning Rate - alpha\n",
    "        alpha = tf.placeholder(tf.float32)\n",
    "        # Dropout Probablity\n",
    "        pkeep = tf.placeholder(tf.float32)\n",
    "\n",
    "        K = 100    # Neurons in 1st hidden layer\n",
    "        L = 75     # Neurons in 2nd hidden layer\n",
    "        M = 50     # Neurons in 3rd hidden layer\n",
    "\n",
    "        Y1 = hidden_layer(X, SIZE, K, name='hidden-1')\n",
    "        Y2 = hidden_layer(Y1, K, L, name='hidden-2')\n",
    "        Y3 = hidden_layer(Y2, L, M, name='hidden-3')\n",
    "        \n",
    "        hidden_list = list()\n",
    "        \n",
    "        for i in range(no_of_hidden_layers):\n",
    "            name = 'hidden-deep' + str(i)\n",
    "            if i == 0:\n",
    "                hidden_list.append(hidden_layer(Y3, M, M, name=name))\n",
    "            else :\n",
    "                y = hidden_list[i-1]\n",
    "                hidden_list.append(hidden_layer(y, M, M, name=name))\n",
    "\n",
    "#         for h in hidden_list:\n",
    "#             print(h.get_shape())\n",
    "#             print(h.name)\n",
    "#         print(hidden_list)\n",
    "                \n",
    "#         print(len(hidden_list))\n",
    "        Y_drop = tf.nn.dropout(hidden_list[-1], pkeep, name='dropout')\n",
    "        YF = hidden_layer(Y_drop, M, HOT, name='output')\n",
    "        Y_f = tf.nn.softmax(YF)\n",
    "\n",
    "        with tf.name_scope('Cross_Entropy'):\n",
    "            cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=Y_f, labels=Y_)) \n",
    "            tf.summary.scalar('Cross_Entropy', cross_entropy)\n",
    "\n",
    "        with tf.name_scope('Train'):\n",
    "            train_step = tf.train.AdamOptimizer(alpha).minimize(cross_entropy)\n",
    "\n",
    "        with tf.name_scope('Accuracy'):\n",
    "            correct_prediction = tf.equal(tf.argmax(Y_f,1), tf.argmax(Y_,1))\n",
    "            accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "            tf.summary.scalar('Accuracy', accuracy)\n",
    "\n",
    "        summary = tf.summary.merge_all()\n",
    "        model_saver = tf.train.Saver()\n",
    "        \n",
    "        \n",
    "    #### TRAINING START ####\n",
    "    batch_size = 512\n",
    "    test_batch = 512\n",
    "    num_steps = int(label_data.shape[0] / batch_size)\n",
    "    num_epochs = int(test_label_hot.shape[0]/test_batch)\n",
    "\n",
    "    with tf.Session(graph=graph_a) as session:\n",
    "        writer = tf.summary.FileWriter(LOGDIR, graph_a)\n",
    "        tf.global_variables_initializer().run()\n",
    "        print('Initialized')\n",
    "\n",
    "        test_acc = list()\n",
    "        train_acc = list()\n",
    "        for epoch in range(num_epochs - 1):\n",
    "            t_data = test_data[epoch*test_batch:(epoch+1)*test_batch]\n",
    "            t_label = test_label_hot[epoch*test_batch:(epoch+1)*test_batch]\n",
    "\n",
    "            for step in range(num_steps - 1):\n",
    "                max_learning_rate = 0.0005\n",
    "                min_learning_rate = 0.0001\n",
    "\n",
    "                decay_speed = 5000.0\n",
    "                learning_rate = min_learning_rate + (max_learning_rate - min_learning_rate) * math.exp(-step/decay_speed)\n",
    "\n",
    "                batch_data = train_data[step*batch_size:(step + 1)*batch_size,:]\n",
    "                batch_labels = label_data[step*batch_size:(step + 1)*batch_size]\n",
    "                feed_dict = {X : batch_data, Y_ : batch_labels, pkeep : 0.75, alpha : learning_rate}\n",
    "\n",
    "                _, l, accc = session.run([train_step, cross_entropy, accuracy], feed_dict=feed_dict)\n",
    "                train_acc.append(accc)\n",
    "\n",
    "            # Test Accuracy\n",
    "            feed_dict = {X : t_data, Y_ : t_label, pkeep : 1.00, alpha : 0.00}\n",
    "            _, l, accc = session.run([train_step, cross_entropy, accuracy], feed_dict=feed_dict)\n",
    "            test_acc.append(accc)\n",
    "        \n",
    "        test_accuracy = np.mean(test_acc)\n",
    "        print('Train Accuract : ', np.mean(train_acc))\n",
    "        print('Test Accuracy : ', test_accuracy)\n",
    "        hidden_dict[no_of_hidden_layers] = test_accuracy\n",
    "        \n",
    "        save_path = model_saver.save(session, 'models/hyp_tune/' + str(no_of_hidden_layers) + '.ckpt')\n",
    "        print(\"Model saved in file: %s\" % save_path)\n",
    "        print('-----------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for i in range(1,10):\n",
    "    test_layer_performance(i)\n",
    "    \n",
    "print hidden_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "file = 'models/hidden_dp_75.pickle'\n",
    "\n",
    "with open(file, 'wb') as handle:\n",
    "    pickle.dump(hidden_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
=======
>>>>>>> refs/remotes/origin/himanshu
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
